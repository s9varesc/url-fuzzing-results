{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details\n",
    "\n",
    "- generation of inputs with [tribble](https://github.com/havrikov/tribble/)\n",
    "- inputs generated from two grammars, one based on the current [URL standard](https://url.spec.whatwg.org/), the other one based on the RFC documents [RFC 3986](https://tools.ietf.org/html/rfc3986#appendix-A) and [RFC 6874](https://tools.ietf.org/html/rfc6874)\n",
    "- experiment executed for both grammars with identical docker images\n",
    "- for each run: 10 inputs selected at random from the generated inputs and added to the tests to execute\n",
    "- run x contains all inputs used in runs 0..x-1 and 10 newly selected inputs\n",
    "- execution of tests for 11 URL parsers:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Language* | *URL Parser* | *Coverage Tool* \n",
    "--- | --- | ---\n",
    "Firefox | nsURLParsers.cpp | grcov/genhtml\n",
    "Chromium | url_parse.cc | coverage.py script\n",
    "C | uriparser | LCOV \n",
    "C++ | POCO |LCOV \n",
    "Go | Package net/url | Package testing with coverage flags \n",
    "Java | java.net URL Class | JCOV \n",
    "JavaScript <br> | urijs <br> jsdom/whatwg-url| nyc/istanbul <br>\n",
    "PHP | League URI | PHPUnit  \n",
    "Python 3| urllib.parse | Coverage.py  \n",
    "Ruby | Module URI |SimpleCov "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas markdown matplotlib\n",
    "from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# view results of 100 runs with single inputs added at \"./smallExp/(ls | rfc)\"\n",
    "ls_df=pd.read_csv(\"./ls/experimentResultsMain.csv\", index_col='run_nr').rename(columns=str.lower)\n",
    "\n",
    "ls_df_comp=pd.read_csv(\"./ls/experimentResultsComponents.csv\", index_col='run_nr').rename(columns=str.lower)\n",
    "\n",
    "rfc_df=pd.read_csv(\"./rfc/experimentResultsMain.csv\", index_col='run_nr').rename(columns=str.lower)\n",
    "rfc_df_comp=pd.read_csv(\"./rfc/experimentResultsComponents.csv\", index_col='run_nr').rename(columns=str.lower)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "parsers=[c[:-4] for c in ls_df.columns.values if \"-cov\" in c]\n",
    "colors_all = ['orange', 'darkblue', 'red', 'yellow', 'lightblue', 'purple', 'beige', 'grey', 'black', 'pink','green']\n",
    "colors_map={}\n",
    "chars = '0123456789ABCDEF'\n",
    "# coloring\n",
    "i=0\n",
    "for p in parsers:\n",
    "    c=colors_all[i] \n",
    "    colors_map[p]=c\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "comp_colors={}\n",
    "for col in ls_df_comp.columns:\n",
    "    if '_success-' in col:\n",
    "        comp_colors[col]='#0'+''.join(random.sample(chars,5))\n",
    "    elif '_success' in col:\n",
    "        comp_colors[col]='#8'+''.join(random.sample(chars,5))\n",
    "    else:\n",
    "        comp_colors[col]='#F'+''.join(random.sample(chars,5))\n",
    "for col in rfc_df_comp.columns:\n",
    "    if col not in comp_colors:\n",
    "        comp_colors[col]='#7'+''.join(random.sample(chars,5))\n",
    "    \n",
    "    \n",
    "colors_comp = ['darkgreen', 'blue']\n",
    "ms=22\n",
    "\n",
    "def setTickDistances(n, axs):\n",
    "    ticks = axs.xaxis.get_ticklocs()\n",
    "    ticklabels = [l.get_text() for l in axs.xaxis.get_ticklabels()]\n",
    "    axs.xaxis.set_ticks(ticks[::n])\n",
    "    axs.xaxis.set_ticklabels(ticklabels[::n])\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of inputs added in each run (10) is large enough to reach the maximum coverage in the initial run for some parsers. To ensure that this is not caused by the test execution, an additional experiment in which single inputs are added was conducted. Results of this experiment can be viewed by using `./smallExp/ls` and `./smallExp/rfc` when loading csv files.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Living Standard Results\n",
    "### Tabular View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ls_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ls_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overviewPlot(df, title):\n",
    "    fig, axs=plt.subplots(figsize=(40,20))\n",
    "    plt.rcParams['font.size']='40'\n",
    "\n",
    "    for p in parsers:\n",
    "        plot=df[p+\"-cov\"].plot(title=title,\n",
    "                       ylim=(0,100), ax=axs,\n",
    "                       color=colors_map[p], style=\".\", ms=2*ms,\n",
    "                      label=p.capitalize(), fontsize=40)\n",
    "    \n",
    "\n",
    "\n",
    "    plot.legend(bbox_to_anchor=(1,-0.2),markerscale=2.)\n",
    "    plot.grid(True)\n",
    "\n",
    "    axs.set_ylabel(\"Coverages\", fontsize=40)\n",
    "    axs.set_xlabel(\"Run\", fontsize=40)\n",
    "    vals = axs.get_yticks()\n",
    "    axs.set_yticklabels(['{:.0f}'.format(x)+\"%\" for x in vals])\n",
    "    \n",
    "    plt.show()\n",
    "    display(Markdown(\"When directly comparing the coverages achieved in different parsers, one should always consider \\\n",
    "                    the number of lines coverable for each parser by simply parsing inputs. The coverages depicted here \\\n",
    "                    are calculated in relation to the total number of lines in each parser.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overviewPlot(ls_df,\"Living Standard Coverages Overview\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO include some details about the parsers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors and Exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorsOverviewPlot(df, title):\n",
    "    fig, axs=plt.subplots(figsize=(20,15))\n",
    "    plt.rcParams['font.size']='30'\n",
    "\n",
    "\n",
    "    new_df=pd.DataFrame()\n",
    "    for c in df.columns:\n",
    "        if 'exceptions' in c:\n",
    "            new_df[c]=ls_df[c]\n",
    "        \n",
    "        \n",
    "\n",
    "    plot=df.tail(1).plot(kind='barh', title=title,\n",
    "                       ax=axs, width=1.9,\n",
    "                       y=[ cn for cn in df.columns if 'exceptions' in cn],\n",
    "                       color=colors_map.values(), rot=0) \n",
    "    \n",
    "    plot.legend(markerscale=2., loc=(1.2,0))\n",
    "    plot.grid(True)\n",
    "\n",
    "\n",
    "    for bar in plot.patches:\n",
    "        plot.annotate(format(bar.get_width(), '.0f'), \n",
    "                    (bar.get_width() ,bar.get_y() + bar.get_height() / 3 ),\n",
    "                    ha='left',  xytext=(5, 0), \n",
    "                   textcoords='offset points', color='dimgrey')\n",
    "\n",
    "\n",
    "    \n",
    "    axs.set_ylabel(\"Run\", fontsize=40)\n",
    "    axs.set_xlabel(\"Exceptions\", fontsize=40)\n",
    "    axs.set_axisbelow(True)\n",
    "    plt.show()\n",
    "    display(Markdown(\"The above diagram compares the number of parsing exceptions encountered for each parser in \\\n",
    "                    the last available run with \"+str(df['nr-inputs'].max()) +\" distinct inputs.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "errorsOverviewPlot(ls_df, \"Living Standard Exceptions Overview\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Equal and Unequal Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalResultsOverview(df, title):\n",
    "    fig, axs=plt.subplots(figsize=(30,15))\n",
    "    plt.rcParams['font.size']='30'\n",
    "\n",
    "    new_df=pd.DataFrame()\n",
    "    b_new_df=pd.DataFrame()\n",
    "    for c in df.columns:\n",
    "        if 'result' in c:\n",
    "            if 'b-' in c:\n",
    "                b_new_df[c]=df[c]\n",
    "            else:\n",
    "                new_df[c]=df[c]\n",
    "    \n",
    "    \n",
    "    plot=new_df.iloc[::df['nr-inputs'].max()//100].plot(kind='bar', stacked=True, ax=axs, rot=0,\n",
    "                     color=['g', 'pink', 'r'], title=title)\n",
    "    \n",
    "    for container, hatch in zip(plot.containers, (\"/\", \".\")):\n",
    "        for patch in container.patches:\n",
    "            patch.set_hatch(hatch)\n",
    "    \n",
    "    n=df['nr-inputs'].max()/100\n",
    "    #setTickDistances(int(n), axs)\n",
    "    \n",
    "    plot.grid(True)\n",
    "    plot.legend(loc='best')\n",
    "    axs.set_ylabel(\"Inputs\", fontsize=40)\n",
    "    axs.set_axisbelow(True)\n",
    "    plt.show()\n",
    "    display(Markdown(\"In the above diagram a ``eq-success-result`` means that all considered parsers successfully parsed \\\n",
    "    the input. A ``eq-fail-result`` means that all parsers rejected the given input and a ``neq-result`` means that \\\n",
    "    some parsers accepted the input as valid while other parsers rejected the input.\"))\n",
    "    return b_new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_b_new_df=equalResultsOverview(ls_df, \"Living Standard Parser Equality\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def browserEquality(df, title):\n",
    "    fig, axs=plt.subplots(figsize=(30,15))\n",
    "    plt.rcParams['font.size']='30'\n",
    "    nr_inputs=sum([df[col].tail(1) for col in df.columns if 'result' in col])\n",
    "    n=int(nr_inputs)//100\n",
    "    plot=df.iloc[::n].plot(kind='bar', stacked=True, rot=0, ax=axs, \n",
    "            color=['g', 'pink', 'orange', 'lightblue', 'r'], title=title)\n",
    "    axs.set_ylabel(\"Inputs\", fontsize=40)\n",
    "    plot.grid(True)\n",
    "    axs.set_axisbelow(True)\n",
    "    for container, hatch in zip(plot.containers, (\"/\", \".\", \"\\\\\", \"x\")):\n",
    "        for patch in container.patches:\n",
    "            patch.set_hatch(hatch)\n",
    "    \n",
    "    plot.grid(True)\n",
    "    plot.legend(loc='best')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browserEquality(ls_b_new_df, \"Living Standard Browser Results Equality\\n\")\n",
    "display(Markdown(\"Much like the previous diagram, this diagram differs between equal and unequal parsing results. \\\n",
    "                    Because only the results of the tested browsers are considered, the diagram can also show whether \\\n",
    "                    the browsers failed on the same components, on different components, or if the results were completely \\\n",
    "                    unequal i.e. one rejection and one component failure.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Browser Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browserComponentComparison(df_comp, title):\n",
    "    fig, axs=plt.subplots(figsize=(40,20))\n",
    "    plt.rcParams['font.size']='30'\n",
    "\n",
    "    new_df=pd.DataFrame()\n",
    "    for c in df_comp.columns:\n",
    "        if 'nr' not in c:\n",
    "            new_df[c]=df_comp[c]\n",
    "    \n",
    "    colors=[comp_colors[name] for name in new_df.columns ]\n",
    "    nr_inputs=sum([new_df[col].tail(1) for col in new_df.columns])\n",
    "    n=int(nr_inputs)//100\n",
    "    plot=new_df.iloc[::n].plot(kind='bar', stacked=True, ax=axs, rot=0, color=colors ,\n",
    "                     title=title)\n",
    "    axs.set_ylabel(\"Inputs\", fontsize=40)\n",
    "    \n",
    "    plot.grid(True)\n",
    "    \n",
    "    try:\n",
    "        plot.legend(loc=(0,-0.4), ncol=len(new_df.columns)//5)\n",
    "    except:\n",
    "        plot.legend(loc='best')\n",
    "    axs.set_axisbelow(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browserComponentComparison(ls_df_comp, \"Living Standard Browser Component Equality\\n\")\n",
    "display(Markdown(\"The above diagram shows in detail which combination of results was observed for the considered browsers.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Separately for each Browser\n",
    "\n",
    "Considering the latest available run for each figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browserComponentDetail(browser, df, run_nr=-1):\n",
    "    \n",
    "    # select the specified run, default: last run\n",
    "    if run_nr < 0 :\n",
    "        df=df.tail(1)\n",
    "    else:\n",
    "        df=df.head(run_nr+1).tail(1)\n",
    "    plt.rcParams['font.size']='20'\n",
    "    fig, axs=plt.subplots( figsize=(25,15))\n",
    "    \n",
    "    \n",
    "    full_df=pd.DataFrame()\n",
    "    for component in ['scheme', 'username', 'password', 'host', 'port', 'path', 'query', 'fragment']:\n",
    "        # count the relevant columns\n",
    "        inputs=int(df['nr_inputs'])\n",
    "        new_df=pd.DataFrame()\n",
    "        \n",
    "        sumcomponent=0\n",
    "        for c in df.columns:\n",
    "            if browser+'_'+component in c:\n",
    "                new_df[c]=df[c]\n",
    "                sumcomponent+=int(df[c])\n",
    "        \n",
    "        new_df['component']=[component]\n",
    "        new_df['other']=[inputs-sumcomponent]\n",
    "        full_df=pd.concat([full_df, new_df], ignore_index=True, sort=True)\n",
    "    display(full_df.head(10))\n",
    "    comp_colors2=comp_colors\n",
    "    comp_colors2['other']='dimgrey'\n",
    "    \n",
    "    colors=[comp_colors[name] for name in full_df.columns if name != 'component']\n",
    "    \n",
    "    plot=full_df.plot( kind='bar', stacked=True, ax=axs,x='component',\n",
    "                         color=colors,\n",
    "                         rot=0, title=browser.capitalize()+\" Details\\n\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the full results of this run [here](./ls/lastRun/resultOverview.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "browserComponentDetail('firefox', ls_df_comp)\n",
    "display(Markdown(\"These graphs only include results where at least one browser had a component mismatch. \\\n",
    "                    Further below are graphs where at least one browser successfully parsed or immediately rejected the input.\"))\n",
    "browserComponentDetail('chromium', ls_df_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing Success and Rejection\n",
    "\n",
    "A parsing success means that the parser accepted the given input as valid URL and all URL components had the expected content. A rejection means that the parser did not accept the given input as valid URL and no component checks were performed. Between these outcomes lies a third possible outcome: the parser accepts the given input as valid URL but at least one component did not have the expected content.\n",
    "\n",
    "The following figures show which result one browser had when the other browser successfully parsed a URL or when it rejected the URL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browserDetail(browser, df, run_nr=-1):\n",
    "    \n",
    "    # select the specified run, default: last run\n",
    "    if run_nr < 0 :\n",
    "        df=df.tail(1)\n",
    "    else:\n",
    "        df=df.head(run_nr+1).tail(1)\n",
    "    plt.rcParams['font.size']='10'\n",
    "    plt.rcParams['figure.figsize'] = (16,7)\n",
    "    fig, axs=plt.subplots(1,2)\n",
    "    #fig2, axs2=plt.subplots( figsize=(8,8))\n",
    "    \n",
    "    full_df=pd.DataFrame()\n",
    "    for component in ['success', 'reject']:\n",
    "        \n",
    "        # count the relevant columns\n",
    "        inputs=int(df['nr_inputs'])\n",
    "        new_df=pd.DataFrame()\n",
    "        \n",
    "        sumcomponent=0\n",
    "        for c in df.columns:\n",
    "            if browser+'_'+component in c:\n",
    "                new_df[c]=df[c]\n",
    "                sumcomponent+=int(df[c])\n",
    "        \n",
    "        new_df['result']=[component]\n",
    "        \n",
    "        full_df=pd.concat([full_df, new_df], ignore_index=True, sort=True)\n",
    "        \n",
    "        \n",
    "    full_df=full_df.set_index('result')\n",
    "    display(full_df)\n",
    "    tr_df=full_df.transpose().sort_values(by=['success', 'reject'])\n",
    "    plot=tr_df.plot( kind='pie',ax=axs[0], \n",
    "                         colormap='Greens', y='success',\n",
    "                         labels=None,\n",
    "                         legend=False)\n",
    "    \n",
    "    plot.legend(loc=(0.2,-0.2), labels=[c for c in tr_df.transpose().columns if tr_df['success'][c]>0],fontsize=12)\n",
    "    \n",
    "    plot2=full_df.transpose().sort_values(by=['reject']).plot( kind='pie',ax=axs[1],\n",
    "                         colormap='YlOrRd', y='reject',\n",
    "                         labels=None, legend=False)\n",
    "    plot2.legend(loc=(0.2,-0.2),labels=[c for c in tr_df.transpose().columns if tr_df['reject'][c]>0], fontsize=12)\n",
    "    display(Markdown(\"### <center>\"+browser.capitalize()+\" Details</center>\\n\"))\n",
    "    plt.show()\n",
    "    display(Markdown(\"Note that these pie charts **only** show the other browsers results for \\\n",
    "    success or reject results of the specified browser. The different component combinations are \\\n",
    "    discussed above.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "browserDetail('firefox', ls_df_comp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browserDetail('chromium', ls_df_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## RFC Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Tabular View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverages Overview Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overviewPlot(rfc_df,\"RFC Coverages Overview\\n\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors and Exceptions Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorsOverviewPlot(rfc_df, \"RFC Exceptions Overview\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equal and Unequal Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_b_new_df=equalResultsOverview(rfc_df, \"RFC Results Parser Equality\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browserEquality(rfc_b_new_df, \"RFC Browser Results Equality\\n\") \n",
    "display(Markdown(\"Because component checks are currently only implemented for ``url-fuzzing/grammars/livingstandard-url.scala`` \\\n",
    "            the above diagram only contains entries where no component checks are required. Similarly, the next diagram \\\n",
    "            only contains entries for parsing success or rejection.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browserComponentComparison(rfc_df_comp, \"RFC Browser Component Equality\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Browser Comparison\n",
    "\n",
    "Considering the last available run. View the full results of this run [here](./rfc/lastRun/resultoverview.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browserDetail('firefox', rfc_df_comp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browserDetail('chromium', rfc_df_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed per Parser Comparison of Grammars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "ffexcov=74.4 # link to reports or extract from reports\n",
    "ffwptcov=84.4\n",
    "chrexcov=83.65\n",
    "chrwptcov=64.29\n",
    "\n",
    "\n",
    "def compareGrammarResults(parser, df1, newcolumn1, df2, newcolumn2):\n",
    "    display(Markdown(\"### \"+parser.capitalize()))\n",
    "    fig, axs=plt.subplots(figsize=(40,20))\n",
    "    plt.rcParams['font.size']='40'\n",
    "\n",
    "    new_df=pd.DataFrame()\n",
    "    new_df[newcolumn1]=df1[parser+\"-cov\"]\n",
    "    new_df[newcolumn2]=df2[parser+\"-cov\"]\n",
    "\n",
    "    plot=new_df.plot(title=parser.capitalize()+\" Coverages\\n\",ylim=(0,100), ax=axs, style=\".\", color=colors_comp, ms=ms)\n",
    "    \n",
    "    plot.grid(True)\n",
    "    \n",
    "\n",
    "    axs.set_ylabel(\"Coverages\", fontsize=40)\n",
    "    axs.set_xlabel(\"Run\", fontsize=40)\n",
    "    otherTests=\"\"\n",
    "    \n",
    "    if parser=='firefox': #TODO: add coverage reports for wpt and existing tests\n",
    "        plot.axhline(y=ffexcov, color='black', linestyle=':', lw=4, label='Existing Test Files')\n",
    "        otherTests+=\"Existing Test Files Coverage: \"+str(ffexcov)+\"%\"\n",
    "        plot.axhline(y=ffwptcov, color='y', linestyle=':', lw=4, label='WPT tests')\n",
    "        otherTests+=\"\\n\\nWeb platform tests Coverage: \"+str(ffwptcov)+\"%\"\n",
    "    if parser=='chromium':\n",
    "        plot.axhline(y=chrexcov, color='black', linestyle=':', lw=4, label='Existing Test Files')\n",
    "        otherTests+=\"Existing Test Files Coverage: \"+str(chrexcov)+\"%\"\n",
    "        plot.axhline(y=chrwptcov, color='y', linestyle=':', lw=4, label='WPT tests')\n",
    "        otherTests+=\"\\n\\nWeb platform tests Coverage: \"+str(chrwptcov)+\"%\"\n",
    "    plot.legend(loc='best',markerscale=2.)\n",
    "    plt.show()\n",
    "    display(new_df.describe())\n",
    "    \n",
    "    display(Markdown(newcolumn1+\" Max Coverage: \"+str(new_df[newcolumn1].max())+\"% \\\n",
    "                     reached in run \"+str(new_df[newcolumn1].idxmax())+\" [full max run results](./ls/\"+parser+\"/resultoverview.html)\"))\n",
    "    display(Markdown(newcolumn2+\" Max Coverage: \"+str(new_df[newcolumn2].max())+\"% \\\n",
    "                     reached in run \"+str(new_df[newcolumn2].idxmax())+\" [full max run results](./rfc/\"+parser+\"/resultoverview.html)\"))\n",
    "    display(Markdown(otherTests))\n",
    "    \n",
    "    return new_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs={}\n",
    "\n",
    "for parser in parsers:\n",
    "    dfs[parser]=compareGrammarResults(parser, ls_df, \"Living Standard\", rfc_df, \"RFC\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
