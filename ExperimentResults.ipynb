{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details\n",
    "\n",
    "- generation of inputs with [tribble](https://github.com/havrikov/tribble/)\n",
    "- inputs generated from two grammars, one based on the current [URL standard](https://url.spec.whatwg.org/), the other one based on the RFC documents [RFC 3986](https://tools.ietf.org/html/rfc3986#appendix-A) and [RFC 6874](https://tools.ietf.org/html/rfc6874)\n",
    "- experiment executed for both grammars with identical docker images\n",
    "- for each run: 10 inputs selected at random from the generated inputs and added to the tests to execute\n",
    "- run x contains all inputs used in runs 0..x-1 and 10 newly selected inputs\n",
    "- execution of tests for 11 URL parsers:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Language* | *URL Parser* | *Coverage Tool* \n",
    "--- | --- | ---\n",
    "Firefox | nsURLParsers.cpp | grcov/genhtml\n",
    "Chromium | url_parse.cc | coverage.py script\n",
    "C | uriparser | LCOV \n",
    "C++ | POCO |LCOV \n",
    "Go | Package net/url | Package testing with coverage flags \n",
    "Java | java.net URL Class | JCOV \n",
    "JavaScript <br> | urijs <br> jsdom/whatwg-url| nyc/istanbul <br>\n",
    "PHP | League URI | PHPUnit  \n",
    "Python 3| urllib.parse | Coverage.py  \n",
    "Ruby | Module URI |SimpleCov "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas markdown matplotlib\n",
    "from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "# view results of 100 runs with single inputs added at \"./smallExp/(ls | rfc)\"\n",
    "ls_df=pd.read_csv(\"./ls/experimentResultsMain.csv\", index_col='run_nr').rename(columns=str.lower)\n",
    "\n",
    "ls_df_comp=pd.read_csv(\"./ls/experimentResultsComponents.csv\", index_col='run_nr').rename(columns=str.lower)\n",
    "\n",
    "rfc_df=pd.read_csv(\"./rfc/experimentResultsMain.csv\", index_col='run_nr').rename(columns=str.lower)\n",
    "rfc_df_comp=pd.read_csv(\"./rfc/experimentResultsComponents.csv\", index_col='run_nr').rename(columns=str.lower)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "\n",
    "parsers=[c[:-4] for c in ls_df.columns.values if \"-cov\" in c]\n",
    "colors_all = ['orange', 'darkblue', 'red', 'yellow', 'lightblue', 'purple', 'beige', 'grey', 'black', 'pink','green']\n",
    "colors_map={}\n",
    "chars = '0123456789ABCDEF'\n",
    "# coloring\n",
    "i=0\n",
    "for p in parsers:\n",
    "    c=colors_all[i] \n",
    "    colors_map[p]=c\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "comp_colors={}\n",
    "for col in ls_df_comp.columns:\n",
    "    if '_success-' in col:\n",
    "        comp_colors[col]='#0'+''.join(random.sample(chars,5))\n",
    "    elif '_success' in col:\n",
    "        comp_colors[col]='#8'+''.join(random.sample(chars,5))\n",
    "    else:\n",
    "        comp_colors[col]='#F'+''.join(random.sample(chars,5))\n",
    "for col in rfc_df_comp.columns:\n",
    "    if col not in comp_colors:\n",
    "        comp_colors[col]='#7'+''.join(random.sample(chars,5))\n",
    "    \n",
    "    \n",
    "colors_comp = ['#2fb553', '#4a76bd']\n",
    "ms=22\n",
    "\n",
    "def setTickDistances(n, axs):\n",
    "    ticks = axs.xaxis.get_ticklocs()\n",
    "    ticklabels = [l.get_text() for l in axs.xaxis.get_ticklabels()]\n",
    "    axs.xaxis.set_ticks(ticks[::n])\n",
    "    axs.xaxis.set_ticklabels(ticklabels[::n])\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of inputs added in each run (10) is large enough to reach the maximum coverage in the initial run for some parsers. To ensure that this is not caused by the test execution, an additional experiment in which single inputs are added was conducted. Results of this experiment can be viewed by using `./smallExp/ls` and `./smallExp/rfc` when loading csv files.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Living Standard Results\n",
    "### Tabular View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ls_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ls_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overviewPlot(df, title, xrange=None):\n",
    "    fig, axs=plt.subplots(figsize=(40,20))\n",
    "    plt.rcParams['font.size']='40'\n",
    "\n",
    "    # apply some nice spacing\n",
    "    if xrange is not None:\n",
    "        a,b=xrange\n",
    "        title+=\" from run \"+str(a)+\" to run \"+str(b)+\" \\n\"\n",
    "        xrange =(a-0.7, b+0.7)\n",
    "    \n",
    "    for p in parsers:\n",
    "        plot=df[p+\"-cov\"].plot(title=title,\n",
    "                       ylim=(0,100),xlim=xrange, ax=axs,\n",
    "                       color=colors_map[p], style=\".\", ms=2*ms,\n",
    "                      label=p.capitalize(), fontsize=40)\n",
    "    \n",
    "\n",
    "\n",
    "    plot.legend(bbox_to_anchor=(1,-0.2),markerscale=2.)\n",
    "    plot.grid(True)\n",
    "\n",
    "    axs.set_ylabel(\"Coverages\", fontsize=40)\n",
    "    axs.set_xlabel(\"Run\", fontsize=40)\n",
    "    \n",
    "    vals = axs.get_yticks()\n",
    "    axs.set_yticklabels(['{:.0f}'.format(x)+\"%\" for x in vals])\n",
    "    \n",
    "    #plt.show()\n",
    "    plt.savefig('test.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    display(Markdown(\"When directly comparing the coverages achieved in different parsers, one should always consider \\\n",
    "                    the number of lines coverable for each parser by simply parsing inputs. The coverages depicted here \\\n",
    "                    are calculated in relation to the total number of lines in each parser.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overviewPlot(ls_df,\"Living Standard Coverages Overview\\n\", )\n",
    "\n",
    "# plotting only selected runs\n",
    "#overviewPlot(ls_df,\"Living Standard Coverages Overview\\n\", (0,10) )\n",
    "#overviewPlot(ls_df,\"Living Standard Coverages Overview\\n\", (100,110) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO include some details about the parsers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors and Exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorsOverviewPlot(df, title):\n",
    "    plt.rcParams['font.size']='30'\n",
    "    fig, axs=plt.subplots(figsize=(20,15))\n",
    "    \n",
    "    new_df=pd.DataFrame()\n",
    "    for c in df.columns:\n",
    "        if 'exceptions' in c:\n",
    "            new_df[c]=df[c]\n",
    "        \n",
    "        \n",
    "\n",
    "    plot=df.tail(1).plot(kind='barh', title=title,\n",
    "                       ax=axs, width=1.9,\n",
    "                       y=[ cn for cn in df.columns if 'exceptions' in cn],\n",
    "                       color=colors_map.values(), rot=0) \n",
    "    \n",
    "    plot.legend(markerscale=2., loc=(1.2,0))\n",
    "    plot.grid(True)\n",
    "\n",
    "\n",
    "    for bar in plot.patches:\n",
    "        plot.annotate(format(bar.get_width(), '.0f'), \n",
    "                    (bar.get_width() ,bar.get_y() + bar.get_height() / 3 ),\n",
    "                    ha='left',  xytext=(5, 0), \n",
    "                   textcoords='offset points', color='dimgrey')\n",
    "\n",
    "\n",
    "    \n",
    "    axs.set_ylabel(\"Run\", fontsize=40)\n",
    "    axs.set_xlabel(\"\\nExceptions\", fontsize=40)\n",
    "    axs.set_axisbelow(True)\n",
    "    plt.savefig('test.pgf')\n",
    "    #plt.show()\n",
    "    display(Markdown(\"The above diagram compares the number of parsing exceptions encountered for each parser in \\\n",
    "                    the last available run with \"+str(df['nr-inputs'].max()) +\" distinct inputs.\"))\n",
    "    display(new_df.tail(1).transpose())\n",
    "    \n",
    "    \n",
    "def errorsOverTime(df, title, xrange=None):\n",
    "    plt.rcParams['font.size']='40'\n",
    "    fig2, axs2=plt.subplots(figsize=(50,20))\n",
    "    \n",
    "    # apply some nice spacing\n",
    "    if xrange is not None:\n",
    "        a,b=xrange\n",
    "        title+=\" from run \"+str(a)+\" to run \"+str(b)+\" \\n\"\n",
    "        xrange =(a-0.7, b+0.7)\n",
    "    \n",
    "    \n",
    "    for p in parsers:\n",
    "        plot2=df[p+\"-exceptions\"].plot(title=title,\n",
    "                        ax=axs2,xlim=xrange,\n",
    "                       color=colors_map[p], style=\".\", ms=ms,\n",
    "                      label=p.capitalize(), fontsize=40)\n",
    "    plot2.grid(True)\n",
    "    plot2.legend(markerscale=2., loc='best')\n",
    "    axs2.set_ylabel(\"Exceptions\\n\", fontsize=40)\n",
    "    axs2.set_xlabel(\"\\nRun\", fontsize=40)\n",
    "    axs2.set_axisbelow(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "errorsOverviewPlot(ls_df, \"Living Standard Exceptions Overview\\n\")\n",
    "errorsOverTime(ls_df, \"Exceptions over time\\n\")\n",
    "# errors within a specific range\n",
    "# errorsOverTime(ls_df, \"Exceptions over time\\n\", (0,100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Equal and Unequal Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalResultsOverview(df, title):\n",
    "    fig, axs=plt.subplots(figsize=(30,15))\n",
    "    plt.rcParams['font.size']='30'\n",
    "\n",
    "    new_df=pd.DataFrame()\n",
    "    b_new_df=pd.DataFrame()\n",
    "    for c in df.columns:\n",
    "        if 'result' in c:\n",
    "            if 'b-' in c:\n",
    "                b_new_df[c]=df[c]\n",
    "            else:\n",
    "                new_df[c]=df[c]\n",
    "    \n",
    "    \n",
    "    plot=new_df.iloc[::df['nr-inputs'].max()//100].plot(kind='bar', stacked=True, ax=axs, rot=0,\n",
    "                     color=['g', 'pink', 'r'], title=title)\n",
    "    \n",
    "    for container, hatch in zip(plot.containers, (\"/\", \".\")):\n",
    "        for patch in container.patches:\n",
    "            patch.set_hatch(hatch)\n",
    "    \n",
    "    n=df['nr-inputs'].max()/100\n",
    "    #setTickDistances(int(n), axs)\n",
    "    \n",
    "    plot.grid(True)\n",
    "    plot.legend(loc='best')\n",
    "    axs.set_ylabel(\"Inputs\", fontsize=40)\n",
    "    axs.set_axisbelow(True)\n",
    "    plt.show()\n",
    "    display(Markdown(\"In the above diagram a ``eq-success-result`` means that all considered parsers successfully parsed \\\n",
    "    the input. A ``eq-fail-result`` means that all parsers rejected the given input and a ``neq-result`` means that \\\n",
    "    some parsers accepted the input as valid while other parsers rejected the input.\"))\n",
    "    return b_new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_b_new_df=equalResultsOverview(ls_df, \"Living Standard Parser Equality\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def browserEquality(df, title):\n",
    "    fig, axs=plt.subplots(figsize=(30,15))\n",
    "    plt.rcParams['font.size']='30'\n",
    "    nr_inputs=sum([df[col].tail(1) for col in df.columns if 'result' in col])\n",
    "    n=int(nr_inputs)//100\n",
    "    plot=df.iloc[::n].plot(kind='bar', stacked=True, rot=0, ax=axs, \n",
    "            color=['g', 'pink', 'orange', 'lightblue', 'r'], title=title)\n",
    "    axs.set_ylabel(\"Inputs\", fontsize=40)\n",
    "    plot.grid(True)\n",
    "    axs.set_axisbelow(True)\n",
    "    for container, hatch in zip(plot.containers, (\"/\", \".\", \"\\\\\", \"x\")):\n",
    "        for patch in container.patches:\n",
    "            patch.set_hatch(hatch)\n",
    "    \n",
    "    plot.grid(True)\n",
    "    plot.legend(loc='best')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browserEquality(ls_b_new_df, \"Living Standard Browser Results Equality\\n\")\n",
    "display(Markdown(\"Much like the previous diagram, this diagram differs between equal and unequal parsing results. \\\n",
    "                    Because only the results of the tested browsers are considered, the diagram can also show whether \\\n",
    "                    the browsers failed on the same components, on different components, or if the results were completely \\\n",
    "                    unequal i.e. one rejection and one component failure.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Browser Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browserComponentComparison(df_comp, title):\n",
    "    fig, axs=plt.subplots(figsize=(45,25))\n",
    "    plt.rcParams['font.size']='30'\n",
    "\n",
    "    new_df=pd.DataFrame()\n",
    "    for c in df_comp.columns:\n",
    "        if 'nr' not in c:\n",
    "            new_df[c]=df_comp[c]\n",
    "    \n",
    "    colors=[comp_colors[name] for name in new_df.columns]\n",
    "    nr_inputs=sum([new_df[col].tail(1) for col in new_df.columns])\n",
    "    n=int(nr_inputs)//100\n",
    "    plot=new_df.iloc[::n].plot(kind='bar', stacked=True, ax=axs, rot=0, color=colors ,\n",
    "                     title=title)\n",
    "    axs.set_ylabel(\"Inputs\", fontsize=40)\n",
    "    \n",
    "    plot.grid(True)\n",
    "    \n",
    "    try:\n",
    "        plot.legend(loc=(0,-0.4), ncol=len(new_df.columns)//5)\n",
    "    except:\n",
    "        plot.legend(loc='best')\n",
    "    axs.set_axisbelow(True)\n",
    "    plt.show()\n",
    "    display(Markdown(\"The above diagram shows in detail which combination of results was \\\n",
    "            observed for the considered browsers. The following table shows the combination \\\n",
    "            of results of the last available run in numerical form.\"))\n",
    "    \n",
    "    trdf=new_df.tail(1).transpose()\n",
    "    display(trdf.sort_values([c for c in trdf.columns], ascending=False).style.format('{:.0f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browserComponentComparison(ls_df_comp, \"Living Standard Browser Component Equality\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Separately for each Browser\n",
    "\n",
    "Considering the latest available run for each figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browserComponentDetail(browser, df, run_nr=-1):\n",
    "    \n",
    "    # select the specified run, default: last run\n",
    "    if run_nr < 0 :\n",
    "        df=df.tail(1)\n",
    "    else:\n",
    "        df=df.head(run_nr+1).tail(1)\n",
    "    plt.rcParams['font.size']='20'\n",
    "    fig, axs=plt.subplots( figsize=(25,15))\n",
    "    \n",
    "    \n",
    "    full_df=pd.DataFrame()\n",
    "    for component in ['scheme', 'username', 'password', 'host', 'port', 'path', 'query', 'fragment']:\n",
    "        # count the relevant columns\n",
    "        inputs=int(df['nr_inputs'])\n",
    "        new_df=pd.DataFrame()\n",
    "        \n",
    "        sumcomponent=0\n",
    "        for c in df.columns:\n",
    "            if browser+'_'+component in c:\n",
    "                new_df[c]=df[c]\n",
    "                sumcomponent+=int(df[c])\n",
    "        \n",
    "        new_df['component']=[component]\n",
    "        new_df['other']=[inputs-sumcomponent]\n",
    "        full_df=pd.concat([full_df, new_df], ignore_index=True, sort=True)\n",
    "    \n",
    "    \n",
    "    comp_colors2=comp_colors\n",
    "    comp_colors2['other']='dimgrey'\n",
    "    \n",
    "    colors=[comp_colors[name] for name in full_df.columns if name != 'component']\n",
    "    \n",
    "    plot=full_df.plot( kind='bar', stacked=True, ax=axs,x='component',\n",
    "                         color=colors,\n",
    "                         rot=0, title=browser.capitalize()+\" Details\\n\")\n",
    "    plt.show()\n",
    "    \n",
    "    full_df=full_df.set_index('component')\n",
    "    format_dict={}\n",
    "    for cl in full_df.select_dtypes(float).columns:\n",
    "        format_dict[cl]='{:.0f}'\n",
    "    display(full_df.head(10).style.format(format_dict))\n",
    "    display(Markdown('Each table row shows which result combinations occured when the given browser had a component \\\n",
    "            mismatch for the specified component. The column \\'other\\' represents all results where the given browser \\\n",
    "            did not have a component mismatch for the specified component.'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the full results of this run [here](./ls/lastRun/resultOverview.html).\n",
    "\n",
    "The following graphs show which parsing result the other browser had when one browser had a component mismatch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "browserComponentDetail('firefox', ls_df_comp)\n",
    "browserComponentDetail('chromium', ls_df_comp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " These graphs only include results where at least one browser had a component mismatch.\n",
    "Further below are graphs where at least one browser successfully parsed or immediately rejected the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing Success and Rejection\n",
    "\n",
    "A parsing success means that the parser accepted the given input as valid URL and all URL components had the expected content. A rejection means that the parser did not accept the given input as valid URL and no component checks were performed. Between these outcomes lies a third possible outcome: the parser accepts the given input as valid URL but at least one component did not have the expected content.\n",
    "\n",
    "The following figures show which result one browser had when the other browser successfully parsed a URL or when it rejected the URL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browserDetail(browser, df, run_nr=-1):\n",
    "    \n",
    "    # select the specified run, default: last run\n",
    "    if run_nr < 0 :\n",
    "        df=df.tail(1)\n",
    "    else:\n",
    "        df=df.head(run_nr+1).tail(1)\n",
    "    plt.rcParams['font.size']='10'\n",
    "    plt.rcParams['figure.figsize'] = (16,7)\n",
    "    fig, axs=plt.subplots(1,2)\n",
    "    #fig2, axs2=plt.subplots( figsize=(8,8))\n",
    "    \n",
    "    full_df=pd.DataFrame()\n",
    "    for component in ['success', 'reject']:\n",
    "        \n",
    "        # count the relevant columns\n",
    "        inputs=int(df['nr_inputs'])\n",
    "        new_df=pd.DataFrame()\n",
    "        \n",
    "        sumcomponent=0\n",
    "        for c in df.columns:\n",
    "            if browser+'_'+component in c:\n",
    "                new_df[c]=df[c]\n",
    "                sumcomponent+=int(df[c])\n",
    "        \n",
    "        new_df['result']=[component]\n",
    "        \n",
    "        full_df=pd.concat([full_df, new_df], ignore_index=True, sort=True)\n",
    "        \n",
    "        \n",
    "    full_df=full_df.set_index('result')\n",
    "    display(full_df.style.format('{:.0f}'))\n",
    "    tr_df=full_df.transpose().sort_values(by=['success', 'reject'])\n",
    "    plot=tr_df.plot( kind='pie',ax=axs[0], \n",
    "                         colormap='Greens', y='success',\n",
    "                         labels=None,\n",
    "                         legend=False)\n",
    "    \n",
    "    plot.legend(loc=(0.2,-0.2), labels=[c for c in tr_df.transpose().columns if tr_df['success'][c]>0],fontsize=12)\n",
    "    \n",
    "    plot2=full_df.transpose().sort_values(by=['reject']).plot( kind='pie',ax=axs[1],\n",
    "                         colormap='YlOrRd', y='reject',\n",
    "                         labels=None, legend=False)\n",
    "    plot2.legend(loc=(0.2,-0.2),labels=[c for c in tr_df.transpose().columns if tr_df['reject'][c]>0], fontsize=12)\n",
    "    display(Markdown(\"### <center>\"+browser.capitalize()+\" Details</center>\\n\"))\n",
    "    plt.show()\n",
    "    display(Markdown(\"Note that these pie charts **only** show the other browsers results for \\\n",
    "    success or reject results of the specified browser. The different component combinations are \\\n",
    "    discussed above.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "browserDetail('firefox', ls_df_comp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browserDetail('chromium', ls_df_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## RFC Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Tabular View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverages Overview Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overviewPlot(rfc_df,\"RFC Coverages Overview\\n\" )\n",
    "# plotting only selected runs\n",
    "#overviewPlot(rfc_df,\"RFC Coverages Overview\\n\", (0,10) )\n",
    "#overviewPlot(rfc_df,\"RFC Coverages Overview\\n\", (100,110) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors and Exceptions Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorsOverviewPlot(rfc_df, \"RFC Exceptions Overview\\n\") \n",
    "errorsOverTime(rfc_df, \"Exceptions over time\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equal and Unequal Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_b_new_df=equalResultsOverview(rfc_df, \"RFC Results Parser Equality\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browserEquality(rfc_b_new_df, \"RFC Browser Results Equality\\n\") \n",
    "display(Markdown(\"Because component checks are currently only implemented for ``url-fuzzing/grammars/livingstandard-url.scala`` \\\n",
    "            the above diagram only contains entries where no component checks are required. Similarly, the next diagram \\\n",
    "            only contains entries for parsing success or rejection.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browserComponentComparison(rfc_df_comp, \"RFC Browser Component Equality\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Browser Comparison\n",
    "\n",
    "Considering the last available run. View the full results of this run [here](./rfc/lastRun/resultoverview.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browserDetail('firefox', rfc_df_comp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browserDetail('chromium', rfc_df_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed per Parser Comparison of Grammars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "ffexcov=74.4 # link to reports or extract from reports\n",
    "ffwptcov=84.4\n",
    "chrexcov=83.65\n",
    "chrwptcov=64.29\n",
    "\n",
    "source_reports={}\n",
    "source_reports[\"chromium\"]=\"chromium/report.html\"\n",
    "source_reports[\"firefox\"]=\"firefox/nsURLParsers.cpp.gcov.html\" \n",
    "source_reports[\"c\"]=\"C/src/UriParse.c.gcov.html\"\n",
    "source_reports[\"cpp\"]=\"Cpp/src/URI.cpp.gcov.html\"\n",
    "source_reports[\"go\"]=\"Go/index.html\"\n",
    "source_reports[\"java\"]=\"Java/java/net/URL.html\" \n",
    "source_reports[\"javascripturijs\"]=\"JavaScript/urijs/URI.js.html\"\n",
    "source_reports[\"javascriptwhatwg-url\"]=\"JavaScript/whatwg-url/whatwg-url/dist/url-state-machine.js.html\"\n",
    "source_reports[\"php\"]=\"PHP/index.html\"\n",
    "source_reports[\"python\"]=\"Python/_usr_lib_python3_6_urllib_parse_py.html\"\n",
    "source_reports[\"ruby\"]=\"Ruby/index.html\"\n",
    "\n",
    "\n",
    "\n",
    "def compareGrammarResults(parser, df1, newcolumn1, df2, newcolumn2, xrange=None):\n",
    "    display(Markdown(\"### \"+parser.capitalize()))\n",
    "    plt.rcParams['font.size']='40'\n",
    "    fig, axs=plt.subplots(figsize=(40,20))\n",
    "    \n",
    "    \n",
    "    title=parser.capitalize()+\" Coverages\\n\"\n",
    "    # apply some nice spacing\n",
    "    if xrange is not None:\n",
    "        a,b=xrange\n",
    "        title+=\" from run \"+str(a)+\" to run \"+str(b)+\" \\n\"\n",
    "        xrange =(a-0.7, b+0.7)\n",
    "\n",
    "    new_df=pd.DataFrame()\n",
    "    new_df[newcolumn1]=df1[parser+\"-cov\"]\n",
    "    new_df[newcolumn2]=df2[parser+\"-cov\"]\n",
    "\n",
    "    plot=new_df.plot(title=title,ylim=(0,100), xlim=xrange,\n",
    "                     ax=axs, style=\".\", color=colors_comp, ms=ms)\n",
    "    \n",
    "    plot.grid(True)\n",
    "    \n",
    "\n",
    "    axs.set_ylabel(\"Coverages\", fontsize=40)\n",
    "    axs.set_xlabel(\"Run\", fontsize=40)\n",
    "    otherTests=\"\"\n",
    "    \n",
    "    if parser=='firefox': #TODO: add coverage reports for wpt and existing tests\n",
    "        plot.axhline(y=ffexcov, color='black', linestyle=':', lw=4, label='Existing Test Files')\n",
    "        otherTests+=\"Existing Test Files Coverage: \"+str(ffexcov)+\"%\"\n",
    "        otherTests+=\" [coverage report](./otherTestResults/firefoxExistingTests/firefox/nsURLParsers.cpp.gcov.html)\"\n",
    "        plot.axhline(y=ffwptcov, color='y', linestyle=':', lw=4, label='WPT tests')\n",
    "        otherTests+=\"\\n\\nWeb platform tests Coverage: \"+str(ffwptcov)+\"%\"\n",
    "        otherTests+=\" [coverage report](./otherTestResults/firefoxWPTReports/firefoxWPTFullURLs/nsURLParsers.cpp.gcov.html)\"\n",
    "    if parser=='chromium':\n",
    "        plot.axhline(y=chrexcov, color='black', linestyle=':', lw=4, label='Existing Test Files')\n",
    "        otherTests+=\"Existing Test Files Coverage: \"+str(chrexcov)+\"%\"\n",
    "        otherTests+=\" [coverage report](./otherTestResults/chromiumExistingTests/chromiumExistingFull/report.html)\"\n",
    "        plot.axhline(y=chrwptcov, color='y', linestyle=':', lw=4, label='WPT tests')\n",
    "        otherTests+=\"\\n\\nWeb platform tests Coverage: \"+str(chrwptcov)+\"%\"\n",
    "        otherTests+=\" [coverage report](./otherTestResults/chromiumWPTReports/chromiumFullWPT/report.html)\"\n",
    "    plot.legend(loc='best',markerscale=2.)\n",
    "    plt.show()\n",
    "    if xrange is not None:\n",
    "        display(Markdown(\"#### Results from run \"+str(a)+\" to run \"+str(b)))\n",
    "        display(new_df[a:b].describe())\n",
    "        display(Markdown(newcolumn1+\" Max Coverage: \"+str(new_df[a:b][newcolumn1].max())+\"% \\\n",
    "                     reached in run \"+str(new_df[a:b][newcolumn1].idxmax())+\" \"))\n",
    "        display(Markdown(newcolumn2+\" Max Coverage: \"+str(new_df[a:b][newcolumn2].max())+\"% \\\n",
    "                     reached in run \"+str(new_df[a:b][newcolumn2].idxmax())+\" \"))\n",
    "        display(Markdown(otherTests))\n",
    "    else:\n",
    "        display(new_df.describe())\n",
    "        \n",
    "    display(Markdown(\"#### Overall max results\"))\n",
    "    display(Markdown(newcolumn1+\" Max Coverage: \"+str(new_df[newcolumn1].max())+\"% \\\n",
    "                     reached in run \"+str(new_df[newcolumn1].idxmax())+\" \\\n",
    "                     \\n\\n[full max run results](./ls/\"+parser+\"/resultoverview.html) \\\n",
    "                     \\n\\n[coverage report](./ls/\"+parser+\"/\"+source_reports[parser]+\")\"))\n",
    "    \n",
    "    display(Markdown(newcolumn2+\" Max Coverage: \"+str(new_df[newcolumn2].max())+\"% \\\n",
    "                     reached in run \"+str(new_df[newcolumn2].idxmax())+\"\\\n",
    "                     \\n\\n[full max run results](./rfc/\"+parser+\"/resultoverview.html)\\\n",
    "                     \\n\\n[coverage report](./rfc/\"+parser+\"/\"+source_reports[parser]+\")\"))\n",
    "    display(Markdown(otherTests))\n",
    "    \n",
    "    return new_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs={}\n",
    "\n",
    "for parser in sorted(parsers):\n",
    "    dfs[parser]=compareGrammarResults(parser, ls_df, \"Living Standard\", rfc_df, \"RFC\")\n",
    "    \n",
    "# focusing on a range\n",
    "#for parser in parsers:\n",
    "#    dfs[parser]=compareGrammarResults(parser, ls_df, \"Living Standard\", rfc_df, \"RFC\", (0,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of inputs added in each run (10) is large enough to reach the maximum coverage in the initial run for some parsers. To ensure that this is not caused by the test execution, an additional experiment in which single inputs are added was conducted. Results of this experiment can be viewed by using `./smallExp/ls` and `./smallExp/rfc` when loading csv files at the beginning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    plt.rcParams['font.size']='40'\n",
    "    fig, axs=plt.subplots(figsize=(50,20))\n",
    "    \n",
    "    overview_df=pd.DataFrame(index=[d for d in dfs], columns=['Living Standard', \"RFC\"])\n",
    "    for p in dfs:\n",
    "        overview_df.loc[p]['Living Standard']=dfs[p].tail(1)['Living Standard'].iloc[0]\n",
    "        overview_df.loc[p]['RFC']=dfs[p].tail(1)['RFC'].iloc[0]\n",
    "    overview_df=overview_df.sort_index()  \n",
    "    plot=overview_df.plot(kind='bar', title=\"Maximum Coverages per Parser\\n\",ylim=(0,100),\n",
    "                          color=colors_comp, ax=axs, alpha=0.85)\n",
    "    plot.grid(True)\n",
    "    \n",
    "    display(overview_df)\n",
    "    #axs.set_ylabel(\"Coverages\", fontsize=40)\n",
    "    vals = axs.get_yticks()\n",
    "    axs.set_yticklabels(['{:.0f}'.format(x)+\"%\" for x in vals])\n",
    "    \n",
    "    labels = [item.get_text() for item in axs.get_xticklabels()]\n",
    "    blabels=[]\n",
    "    for label in labels:\n",
    "        blabels+=[\"\\n\"+label.replace('script', 'script\\n').capitalize()]\n",
    "    axs.set_xticklabels(blabels)\n",
    "    plt.xticks(rotation=0, horizontalalignment=\"center\")\n",
    "    axs.set_axisbelow(True)\n",
    "    plt.show()\n",
    "    \n",
    "    for c in overview_df.columns:\n",
    "        overview_df[c]=pd.to_numeric(overview_df[c])\n",
    "   \n",
    "    display(overview_df.describe())\n",
    "    \n",
    "    \n",
    "    display(overview_df.sort_values(by='Living Standard', ascending=False))\n",
    "    display(overview_df.sort_values(by='RFC', ascending=False))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
