{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details\n",
    "\n",
    "- generation of inputs with [tribble](https://github.com/havrikov/tribble/)\n",
    "- inputs generated from two grammars, one based on the current [URL standard](https://url.spec.whatwg.org/), the other one based on the RFC documents [RFC 3986](https://tools.ietf.org/html/rfc3986#appendix-A) and [RFC 6874](https://tools.ietf.org/html/rfc6874)\n",
    "- experiment executed for both grammars with identical docker images\n",
    "- for each run: 10 inputs selected at random from the generated inputs and added to the tests to execute\n",
    "- run x contains all inputs used in runs 0..x-1 and 10 newly selected inputs\n",
    "- execution of tests for 11 URL parsers:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Language* | *URL Parser* | *Coverage Tool* \n",
    "--- | --- | ---\n",
    "Firefox | nsURLParsers.cpp | grcov/genhtml\n",
    "Chromium | url_parse.cc | coverage.py script\n",
    "C | uriparser | LCOV \n",
    "C++ | POCO |LCOV \n",
    "Go | Package net/url | Package testing with coverage flags \n",
    "Java | java.net URL Class | JCOV \n",
    "JavaScript <br> | urijs <br> jsdom/whatwg-url| nyc/istanbul <br>\n",
    "PHP | League URI | PHPUnit  \n",
    "Python 3| urllib.parse | Coverage.py  \n",
    "Ruby | Module URI |SimpleCov "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas markdown matplotlib\n",
    "from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "ls_df=pd.read_csv(\"./ls/experimentResultsMain.csv\", index_col='run_nr').rename(columns=str.lower)\n",
    "\n",
    "ls_df_comp=pd.read_csv(\"./ls/experimentResultsComponents.csv\", index_col='run_nr').rename(columns=str.lower)\n",
    "\n",
    "rfc_df=pd.read_csv(\"./rfc/experimentResultsMain.csv\", index_col='run_nr').rename(columns=str.lower)\n",
    "rfc_df_comp=pd.read_csv(\"./rfc/experimentResultsComponents.csv\", index_col='run_nr').rename(columns=str.lower)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "parsers=[c[:-4] for c in ls_df.columns.values if \"-cov\" in c]\n",
    "colors_all = ['orange', 'darkblue', 'red', 'yellow', 'lightblue', 'purple', 'beige', 'grey', 'black', 'pink','green']\n",
    "colors_map={}\n",
    "chars = '0123456789ABCDEF'\n",
    "# coloring\n",
    "i=0\n",
    "for p in parsers:\n",
    "    c=colors_all[i] \n",
    "    colors_map[p]=c\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "comp_colors={}\n",
    "for col in ls_df_comp.columns:\n",
    "    if '_success-' in col:\n",
    "        comp_colors[col]='#0'+''.join(random.sample(chars,5))\n",
    "    elif '_success' in col:\n",
    "        comp_colors[col]='#8'+''.join(random.sample(chars,5))\n",
    "    else:\n",
    "        comp_colors[col]='#F'+''.join(random.sample(chars,5))\n",
    "for col in rfc_df_comp.columns:\n",
    "    if col not in comp_colors:\n",
    "        comp_colors[col]='#7'+''.join(random.sample(chars,5))\n",
    "    \n",
    "    \n",
    "colors_comp = ['darkgreen', 'blue']\n",
    "ms=22\n",
    "\n",
    "def setTickDistances(n, axs):\n",
    "    ticks = axs.xaxis.get_ticklocs()\n",
    "    ticklabels = [l.get_text() for l in axs.xaxis.get_ticklabels()]\n",
    "    axs.xaxis.set_ticks(ticks[::n])\n",
    "    axs.xaxis.set_ticklabels(ticklabels[::n])\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Living Standard Results\n",
    "### Tabular View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ls_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ls_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overviewPlot(df, title):\n",
    "    fig, axs=plt.subplots(figsize=(40,20))\n",
    "    plt.rcParams['font.size']='40'\n",
    "\n",
    "    for p in parsers:\n",
    "        plot=df[p+\"-cov\"].plot(title=title,\n",
    "                       ylim=(0,100), ax=axs,\n",
    "                       color=colors_map[p], style=\".\", ms=2*ms,\n",
    "                      label=p.capitalize())\n",
    "    \n",
    "\n",
    "\n",
    "    plot.legend(bbox_to_anchor=(1,-0.2),markerscale=2.)\n",
    "    plot.grid(True)\n",
    "\n",
    "    axs.set_ylabel(\"Coverages\", fontsize=40)\n",
    "    axs.set_xlabel(\"Run\", fontsize=40)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overviewPlot(ls_df,\"Living Standard Coverages Overview\\n\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors and Exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorsOverviewPlot(df, title):\n",
    "    fig, axs=plt.subplots(figsize=(7,10))\n",
    "    plt.rcParams['font.size']='30'\n",
    "\n",
    "\n",
    "    new_df=pd.DataFrame()\n",
    "    for c in df.columns:\n",
    "        if 'exceptions' in c:\n",
    "            new_df[c]=ls_df[c]\n",
    "        \n",
    "        \n",
    "\n",
    "    plot=df.tail(1).plot(kind='bar', title=title,\n",
    "                       ax=axs, width=0.9,\n",
    "                       y=[ cn for cn in df.columns if 'exceptions' in cn],\n",
    "                       color=colors_map.values(), rot=0) \n",
    "    \n",
    "    plot.legend(markerscale=2., loc=(1.2,0))\n",
    "    plot.grid(True)\n",
    "\n",
    "\n",
    "    for bar in plot.patches:\n",
    "        plot.annotate(format(bar.get_height(), '.0f'), \n",
    "                   (bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "                    ha='center',  xytext=(0, 10),\n",
    "                   textcoords='offset points', color='dimgrey')\n",
    "\n",
    "\n",
    "    \n",
    "    axs.set_ylabel(\"Exceptions\", fontsize=40)\n",
    "    axs.set_xlabel(\"Run\", fontsize=40)\n",
    "    axs.set_axisbelow(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "errorsOverviewPlot(ls_df, \"Living Standard Exceptions Overview\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Equal and Unequal Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalResultsOverview(df, title):\n",
    "    fig, axs=plt.subplots(figsize=(30,15))\n",
    "    plt.rcParams['font.size']='30'\n",
    "\n",
    "    new_df=pd.DataFrame()\n",
    "    b_new_df=pd.DataFrame()\n",
    "    for c in df.columns:\n",
    "        if 'result' in c:\n",
    "            if 'b-' in c:\n",
    "                b_new_df[c]=df[c]\n",
    "            else:\n",
    "                new_df[c]=df[c]\n",
    "    \n",
    "    \n",
    "    plot=new_df.plot(kind='bar', stacked=True, ax=axs, rot=0,\n",
    "                     color=['g', 'pink', 'r'], title=title)\n",
    "    \n",
    "    for container, hatch in zip(plot.containers, (\"/\", \".\")):\n",
    "        for patch in container.patches:\n",
    "            patch.set_hatch(hatch)\n",
    "    \n",
    "    n=df['nr-inputs'].max()//10\n",
    "    setTickDistances(n, axs)\n",
    "    \n",
    "    plot.grid(True)\n",
    "    plot.legend(loc='best')\n",
    "    axs.set_ylabel(\"Inputs\", fontsize=40)\n",
    "    axs.set_axisbelow(True)\n",
    "    plt.show()\n",
    "    return b_new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_b_new_df=equalResultsOverview(ls_df, \"Living Standard Parser Equality\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def browserEquality(df, title):\n",
    "    fig, axs=plt.subplots(figsize=(30,15))\n",
    "    plt.rcParams['font.size']='30'\n",
    "    plot=df.plot(kind='bar', stacked=True, rot=0, ax=axs, \n",
    "            color=['g', 'pink', 'orange', 'lightblue', 'r'], title=title)\n",
    "    axs.set_ylabel(\"Inputs\", fontsize=40)\n",
    "    plot.grid(True)\n",
    "    axs.set_axisbelow(True)\n",
    "    for container, hatch in zip(plot.containers, (\"/\", \".\", \"\\\\\", \"x\")):\n",
    "        for patch in container.patches:\n",
    "            patch.set_hatch(hatch)\n",
    "    nr_inputs=sum([df[col].tail(1) for col in df.columns if 'result' in col])\n",
    "    n=int(nr_inputs)//10\n",
    "    setTickDistances(n, axs)\n",
    "    plot.grid(True)\n",
    "    plot.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browserEquality(ls_b_new_df, \"Living Standard Browser Results Equality\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Browser Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browserComponentComparison(df_comp, title):\n",
    "    fig, axs=plt.subplots(figsize=(30,15))\n",
    "    plt.rcParams['font.size']='30'\n",
    "\n",
    "    new_df=pd.DataFrame()\n",
    "    for c in df_comp.columns:\n",
    "        if 'nr' not in c:\n",
    "            new_df[c]=df_comp[c]\n",
    "    \n",
    "    colors=[comp_colors[name] for name in new_df.columns ]\n",
    "    \n",
    "    plot=new_df.plot(kind='bar', stacked=True, ax=axs, rot=0, color=colors ,\n",
    "                     title=title)\n",
    "    axs.set_ylabel(\"Inputs\", fontsize=40)\n",
    "    nr_inputs=sum([new_df[col].tail(1) for col in new_df.columns])\n",
    "    n=int(nr_inputs)//10\n",
    "    setTickDistances(n, axs)\n",
    "    plot.grid(True)\n",
    "    axs.set_axisbelow(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browserComponentComparison(ls_df_comp, \"Living Standard Browser Component Equality\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Separately for each Browser\n",
    "\n",
    "Considering the latest available run for each figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browserComponentDetail(browser, df, run_nr=-1):\n",
    "    \n",
    "    # select the specified run, default: last run\n",
    "    if run_nr < 0 :\n",
    "        df=df.tail(1)\n",
    "    else:\n",
    "        df=df.head(run_nr+1).tail(1)\n",
    "    fig, axs=plt.subplots( figsize=(25,15))\n",
    "    plt.rcParams['font.size']='30'\n",
    "    \n",
    "    full_df=pd.DataFrame()\n",
    "    for component in ['scheme', 'username', 'password', 'host', 'port', 'path', 'query', 'fragment']:\n",
    "        # count the relevant columns\n",
    "        inputs=int(df['nr_inputs'])\n",
    "        new_df=pd.DataFrame()\n",
    "        \n",
    "        sumcomponent=0\n",
    "        for c in df.columns:\n",
    "            if browser+'_'+component in c:\n",
    "                new_df[c]=df[c]\n",
    "                sumcomponent+=int(df[c])\n",
    "        \n",
    "        new_df['component']=[component]\n",
    "        new_df['other']=[inputs-sumcomponent]\n",
    "        full_df=pd.concat([full_df, new_df], ignore_index=True, sort=True)\n",
    "    display(full_df.head(10))\n",
    "    comp_colors2=comp_colors\n",
    "    comp_colors2['other']='dimgrey'\n",
    "    \n",
    "    colors=[comp_colors[name] for name in full_df.columns if name != 'component']\n",
    "    \n",
    "    plot=full_df.plot( kind='bar', stacked=True, ax=axs,x='component',\n",
    "                         color=colors,\n",
    "                         rot=0, title=browser.capitalize()+\" Details\\n\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the full results of this run [here](./ls/lastRun/resultOverview.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "browserComponentDetail('firefox', ls_df_comp)\n",
    "browserComponentDetail('chromium', ls_df_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing Success and Rejection\n",
    "\n",
    "A parsing success means that the parser accepted the given input as valid URL and all URL components had the expected content. A rejection means that the parser did not accept the given input as valid URL and no component checks were performed. Between these outcomes lies a third possible outcome: the parser accepts the given input as valid URL but at least one component did not have the expected content.\n",
    "\n",
    "The following figures show which result one browser had when the other browser successfully parsed a URL or when it rejected the URL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browserDetail(browser, df, run_nr=-1):\n",
    "    \n",
    "    # select the specified run, default: last run\n",
    "    if run_nr < 0 :\n",
    "        df=df.tail(1)\n",
    "    else:\n",
    "        df=df.head(run_nr+1).tail(1)\n",
    "    fig, axs=plt.subplots( figsize=(10,10))\n",
    "    #plt.rcParams['font.size']='30'\n",
    "    fig2, axs2=plt.subplots( figsize=(10,10))\n",
    "    \n",
    "    full_df=pd.DataFrame()\n",
    "    for component in ['success', 'reject']:\n",
    "        \n",
    "        # count the relevant columns\n",
    "        inputs=int(df['nr_inputs'])\n",
    "        new_df=pd.DataFrame()\n",
    "        \n",
    "        sumcomponent=0\n",
    "        for c in df.columns:\n",
    "            if browser+'_'+component in c:\n",
    "                new_df[c]=df[c]\n",
    "                sumcomponent+=int(df[c])\n",
    "        \n",
    "        new_df['result']=[component]\n",
    "        \n",
    "        full_df=pd.concat([full_df, new_df], ignore_index=True, sort=True)\n",
    "        \n",
    "        \n",
    "    full_df=full_df.set_index('result')\n",
    "    display(full_df)    \n",
    "    plot=full_df.transpose().plot( kind='pie',ax=axs,\n",
    "                         colormap='Greens', y='success',\n",
    "                         title=browser.capitalize()+\" Details\\n\",\n",
    "                        legend=False)\n",
    "    \n",
    "    plot2=full_df.transpose().plot( kind='pie',ax=axs2,\n",
    "                         colormap='YlOrRd', y='reject',\n",
    "                        legend=False)\n",
    "    \n",
    "    plt.show()\n",
    "    display(Markdown(\"Note that these pie charts **only** show the other browsers results for \\\n",
    "    success or reject results of the specified browser. The different component combinations are \\\n",
    "    discussed above.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "browserDetail('firefox', ls_df_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browserDetail('chromium', ls_df_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## RFC Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Tabular View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverages Overview Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overviewPlot(rfc_df,\"RFC Coverages Overview\\n\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors and Exceptions Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorsOverviewPlot(rfc_df, \"RFC Exceptions Overview\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equal and Unequal Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_b_new_df=equalResultsOverview(rfc_df, \"RFC Results Parser Equality\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browserEquality(rfc_b_new_df, \"RFC Browser Results Equality\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browserComponentComparison(rfc_df_comp, \"RFC Browser Component Equality\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Browser Comparison\n",
    "\n",
    "Considering the last available run. View the full results of this run [here](./rfc/lastRun/resultoverview.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browserDetail('firefox', rfc_df_comp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browserDetail('chromium', rfc_df_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed per Parser Comparison of Grammars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "ffexcov=74.4 # link to reports or extract from reports\n",
    "ffwptcov=84.4\n",
    "chrexcov=83.65\n",
    "chrwptcov=64.29\n",
    "\n",
    "\n",
    "def compareGrammarResults(parser, df1, newcolumn1, df2, newcolumn2):\n",
    "    display(Markdown(\"### \"+parser.capitalize()))\n",
    "    fig, axs=plt.subplots(figsize=(40,20))\n",
    "    plt.rcParams['font.size']='40'\n",
    "\n",
    "    new_df=pd.DataFrame()\n",
    "    new_df[newcolumn1]=df1[parser+\"-cov\"]\n",
    "    new_df[newcolumn2]=df2[parser+\"-cov\"]\n",
    "\n",
    "    plot=new_df.plot(title=parser.capitalize()+\" Coverages\\n\",ylim=(0,100), ax=axs, style=\".\", color=colors_comp, ms=ms)\n",
    "    \n",
    "    plot.grid(True)\n",
    "    \n",
    "\n",
    "    axs.set_ylabel(\"Coverages\", fontsize=40)\n",
    "    axs.set_xlabel(\"Run\", fontsize=40)\n",
    "    otherTests=\"\"\n",
    "    \n",
    "    if parser=='firefox': #TODO: add coverage reports for wpt and existing tests\n",
    "        plot.axhline(y=ffexcov, color='black', linestyle=':', lw=4, label='Existing Test Files')\n",
    "        otherTests+=\"Existing Test Files Coverage: \"+str(ffexcov)+\"%\"\n",
    "        plot.axhline(y=ffwptcov, color='y', linestyle=':', lw=4, label='WPT tests')\n",
    "        otherTests+=\"\\n\\nWeb platform tests Coverage: \"+str(ffwptcov)+\"%\"\n",
    "    if parser=='chromium':\n",
    "        plot.axhline(y=chrexcov, color='black', linestyle=':', lw=4, label='Existing Test Files')\n",
    "        otherTests+=\"Existing Test Files Coverage: \"+str(chrexcov)+\"%\"\n",
    "        plot.axhline(y=chrwptcov, color='y', linestyle=':', lw=4, label='WPT tests')\n",
    "        otherTests+=\"\\n\\nWeb platform tests Coverage: \"+str(chrwptcov)+\"%\"\n",
    "    plot.legend(loc='best',markerscale=2.)\n",
    "    plt.show()\n",
    "    display(new_df.describe())\n",
    "    \n",
    "    display(Markdown(newcolumn1+\" Max Coverage: \"+str(new_df[newcolumn1].max())+\"% \\\n",
    "                     reached in run \"+str(new_df[newcolumn1].idxmax())+\" [full max run results](./ls/\"+parser+\"/resultoverview.html)\"))\n",
    "    display(Markdown(newcolumn2+\" Max Coverage: \"+str(new_df[newcolumn2].max())+\"% \\\n",
    "                     reached in run \"+str(new_df[newcolumn2].idxmax())+\" [full max run results](./rfc/\"+parser+\"/resultoverview.html)\"))\n",
    "    display(Markdown(otherTests))\n",
    "    \n",
    "    return new_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs={}\n",
    "\n",
    "for parser in parsers:\n",
    "    dfs[parser]=compareGrammarResults(parser, ls_df, \"Living Standard\", rfc_df, \"RFC\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
